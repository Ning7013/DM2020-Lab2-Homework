{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:NING KUO HSUN\n",
    "\n",
    "Student ID:107062625\n",
    "\n",
    "GitHub ID:Ning7013\n",
    "\n",
    "Kaggle name: .S. Ning\n",
    "\n",
    "Kaggle private scoreboard snapshot: ![png](img/score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2020-Lab2-Master Repo](https://github.com/fhcalderon87/DM2020-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2020-hw2-nthu/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 5th 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM2020-Lab2-Homework](https://github.com/fhcalderon87/DM2020-Lab2-Homework) repository this time! Also please __DONÂ´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "#train set\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_df.text)\n",
    "term_frequencies = []\n",
    "for j in range(0,train_counts.shape[1]):\n",
    "    term_frequencies.append(sum(train_counts[:,j].toarray()))\n",
    "term_frequencies = np.asarray(train_counts.sum(axis=0))[0]\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "df = df.sort_values(by=['frequency'],ascending = False)\n",
    "df = df.head(30)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "test_counts = count_vect.fit_transform(test_df.text)\n",
    "term_frequencies = []\n",
    "for j in range(0,test_counts.shape[1]):\n",
    "    term_frequencies.append(sum(test_counts[:,j].toarray()))\n",
    "term_frequencies = np.asarray(test_counts.sum(axis=0))[0]\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "df = df.sort_values(by=['frequency'],ascending = False)\n",
    "df = df.head(30)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "#tf-idf part\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "tfidf_1k = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "tfidf_1k.fit(train_df['text'])\n",
    "train_data_tfidf_features_1k = tfidf_1k.transform(train_df['text'])\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "feature_names_1k = tfidf_1k.get_feature_names()\n",
    "feature_names_1k[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (Take home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows which kind of emotion often be judge as other kind of emotion. As a example, a sentence of anger except to itself,\n",
    "it is mostly seen as fear. It somehow reflects how similar a emotion is to another emotion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (Take home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by decision tree: array(['fear', 'anger', 'joy', 'anger', 'joy', 'sadness', 'fear', 'joy', 'fear', 'fear'], dtype=object)\n",
    "by naive bayes: array(['fear', 'anger', 'fear', 'anger', 'joy', 'anger', 'fear', 'joy', 'fear', 'sadness'], dtype='<U7')\n",
    "\n",
    "mostly they get same result, in the different result, there might be some key words such that decision tree algorithm think it\n",
    "should be classified as the result, but naive bayes think it still has a better chance to be another result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0741eac7d034>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#testing acc. stop getting higher at second epoch, and testing loss stop dropping at second or third epoch, means that it has\n",
    "#serious underfitting problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (Take home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends. When using some models like bag of words, it just count each word. and when there is context-awarness, some encoding\n",
    "technique will be needed, such as one hot encoding. And for more advance model, like LSTM, it remembers some previous\n",
    "information, then predict what the upcoming word should be, and optionally ignore some words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_google_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e766ac3b7218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtopn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhappy_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'happy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'happy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mangry_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'angry'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'angry'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msad_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sad'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v_google_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_list = ['happy', 'angry', 'sad', 'fear']\n",
    "\n",
    "topn = 15\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "sad_words = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \n",
    "fear_words = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('sad_words: ', sad_words)\n",
    "print('fear_words: ', fear_words)\n",
    "\n",
    "target_words = happy_words + angry_words + sad_words + fear_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#read in files and fit it into pandas dataframe\n",
    "raw_data = pd.read_json(\"/kaggle/input/dm2020hw2nthu/tweets_DM.json\",lines=True,orient='columns')\n",
    "src = pd.DataFrame([md for md in raw_data._source])\n",
    "src = pd.DataFrame([md for md in src.tweet])\n",
    "\n",
    "#read in tags(if exist)\n",
    "df = pd.read_csv('/kaggle/input/dm2020hw2nthu/data_identification.csv', delimiter=',')\n",
    "src = pd.merge(src, df, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "df = pd.read_csv('/kaggle/input/dm2020hw2nthu/emotion.csv', delimiter=',')\n",
    "src = pd.merge(src, df, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "\n",
    "#seperate test and train data\n",
    "test = src.groupby('identification').get_group('test')\n",
    "train = src.groupby('identification').get_group('train')\n",
    "test = test.drop(['hashtags', 'identification', 'emotion'], axis=1)\n",
    "train = train.drop(['hashtags', 'identification'], axis=1)\n",
    "\n",
    "#left only few records for time result. I did it on kaggle GPU so only few hours of training time can be used.\n",
    "train = train.iloc[:500191]\n",
    "\n",
    "#some preprocessing for text fiels to fit bert's needs\n",
    "#remove URL\n",
    "train['text'] = train['text'].str.replace(r'http(\\S)+', r'')\n",
    "train['text'] = train['text'].str.replace(r'http ...', r'')\n",
    "train['text'] = train['text'].str.replace(r'http', r'')\n",
    "train[train['text'].str.contains(r'http')]\n",
    "\n",
    "# remove RT, @\n",
    "train['text'] = train['text'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "train[train['text'].str.contains(r'RT[ ]?@')]\n",
    "train['text'] = train['text'].str.replace(r'@[\\S]+',r'')\n",
    "\n",
    "#remove non-ascii words and characters\n",
    "train['text'] = [''.join([i if ord(i) < 128 else '' for i in text]) for text in train['text']]\n",
    "train['text'] = train['text'].str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "#remove &, < and >\n",
    "train['text'] = train['text'].str.replace(r'&amp;?',r'and')\n",
    "train['text'] = train['text'].str.replace(r'&lt;',r'<')\n",
    "train['text'] = train['text'].str.replace(r'&gt;',r'>')\n",
    "\n",
    "# remove extra space\n",
    "train['text'] = train['text'].str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "# insert space between punctuation marks\n",
    "train['text'] = train['text'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "train['text'] = train['text'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "# lower case and strip white spaces at both ends\n",
    "train['text'] = train['text'].str.lower()\n",
    "train['text'] = train['text'].str.strip()\n",
    "\n",
    "#also fit test's text field. \n",
    "#remove URL\n",
    "test['text'] = test['text'].str.replace(r'http(\\S)+', r'')\n",
    "test['text'] = test['text'].str.replace(r'http ...', r'')\n",
    "test['text'] = test['text'].str.replace(r'http', r'')\n",
    "test[test['text'].str.contains(r'http')]\n",
    "\n",
    "# remove RT, @\n",
    "test['text'] = test['text'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "test[test['text'].str.contains(r'RT[ ]?@')]\n",
    "test['text'] = test['text'].str.replace(r'@[\\S]+',r'')\n",
    "\n",
    "#remove non-ascii words and characters\n",
    "test['text'] = [''.join([i if ord(i) < 128 else '' for i in text]) for text in test['text']]\n",
    "test['text'] = test['text'].str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "#remove &, < and >\n",
    "test['text'] = test['text'].str.replace(r'&amp;?',r'and')\n",
    "test['text'] = test['text'].str.replace(r'&lt;',r'<')\n",
    "test['text'] = test['text'].str.replace(r'&gt;',r'>')\n",
    "\n",
    "# remove extra space\n",
    "test['text'] = test['text'].str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "# insert space between punctuation marks\n",
    "test['text'] = test['text'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "test['text'] = test['text'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "# lower case and strip white spaces at both ends\n",
    "test['text'] = test['text'].str.lower()\n",
    "test['text'] = test['text'].str.strip()\n",
    "\n",
    "\n",
    "#encode emtions\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "train['emotion_enc'] = labelencoder.fit_transform(train['emotion'])\n",
    "\n",
    "#save it for later use.\n",
    "test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#load bert model from huggingface\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "\n",
    "#setup bert's dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ProcessDataset(Dataset):\n",
    "    # read tsv and initialize some param.\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # seoerate train and test routine\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(\"./\"+mode + \".tsv\", sep=\"\\t\",lineterminator='\\n').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4,\n",
    "                          'sadness': 5, 'surprise': 6, 'trust': 7}\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # func. to return result\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text = self.df.iloc[idx, 1:2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text, label = self.df.iloc[idx, 1:3].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # fit bert's need\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(str(text))\n",
    "        word_pieces += tokens + [\"[SEP]\"]\n",
    "        len_0 = len(word_pieces)\n",
    "        \n",
    "        # turn whole token into bert's index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * len_0, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "trainset = ProcessDataset(\"train\", tokenizer=tokenizer)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#input a list \"smaples\", where every element in it is a return value from ProcessDataset. each smaple contains three tensors:\n",
    "# tokens_tensor\n",
    "# segments_tensor\n",
    "# label_tensor\n",
    "#it zero-padding top two tensors, creating mask tensor.\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # only testset are labeled.\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks, only none zero padding part should be focused on.\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 8\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # move tensors on CUDA\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # get accuracy\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            #record current batch\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "#use gpu if able\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "#uss adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 4\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #record batch loss\n",
    "        running_loss += loss.item()\n",
    "    #calculate accuracy\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))\n",
    "\n",
    "#try on testset\n",
    "testset = ProcessDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n",
    "\n",
    "predictions = get_predictions(model, testloader)\n",
    "\n",
    "#turn emotion from label back string\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "#generate file for kaggle\n",
    "df = pd.DataFrame({\"Category\": predictions.tolist()})\n",
    "df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"tweet_id\"]], \n",
    "                          df.loc[:, 'Category']], axis=1)\n",
    "df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
